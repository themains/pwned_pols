{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "593176e0",
   "metadata": {},
   "source": [
    "### UP Legislative Assembly (18)\n",
    "\n",
    "https://uplegisassembly.gov.in/Members/main_members_en.aspx#/ElectedMembers/18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5acee631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bb89424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename='mla_scraping.log'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a28fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLAFirefoxScraper:\n",
    "    def __init__(self):\n",
    "        self.output_file = 'mla_profiles.csv'\n",
    "        self.state_file = 'scraper_state.json'\n",
    "        self.start_id = 18001\n",
    "        self.end_id = 18413\n",
    "        self.driver = self.setup_driver()\n",
    "\n",
    "    def setup_driver(self):\n",
    "        \"\"\"Set up Firefox driver with proper options.\"\"\"\n",
    "        firefox_options = Options()\n",
    "        firefox_options.add_argument('--headless')  # Run in headless mode\n",
    "        firefox_options.add_argument('--width=1920')\n",
    "        firefox_options.add_argument('--height=1080')\n",
    "        \n",
    "        # Set preferences to make the browser more stable\n",
    "        firefox_options.set_preference(\"browser.download.folderList\", 2)\n",
    "        firefox_options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "        firefox_options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/x-gzip\")\n",
    "        firefox_options.set_preference(\"general.useragent.override\", \n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0\")\n",
    "        \n",
    "        return webdriver.Firefox(options=firefox_options)\n",
    "\n",
    "    def load_state(self):\n",
    "        \"\"\"Load the last known state of the scraper.\"\"\"\n",
    "        if os.path.exists(self.state_file):\n",
    "            try:\n",
    "                with open(self.state_file, 'r') as f:\n",
    "                    state = json.load(f)\n",
    "                logging.info(f\"Loaded state: Last processed MLA ID = {state.get('last_processed_id')}\")\n",
    "                return state\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error loading state file: {e}\")\n",
    "        return {'last_processed_id': self.start_id - 1, 'processed_ids': []}\n",
    "\n",
    "    def save_state(self, last_id, processed_ids):\n",
    "        \"\"\"Save the current state of the scraper.\"\"\"\n",
    "        try:\n",
    "            with open(self.state_file, 'w') as f:\n",
    "                json.dump({\n",
    "                    'last_processed_id': last_id,\n",
    "                    'processed_ids': list(processed_ids),\n",
    "                    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }, f)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving state: {e}\")\n",
    "\n",
    "    def scrape_mla_profile(self, mla_id):\n",
    "        \"\"\"Scrape individual MLA profile using Selenium.\"\"\"\n",
    "        url = f\"https://uplegisassembly.gov.in/Members/mla_profile_hi.aspx?mla_id={mla_id}&assembly_no=18\"\n",
    "        \n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            # Wait for the main content to load\n",
    "            WebDriverWait(self.driver, 20).until(\n",
    "                EC.presence_of_element_located((By.ID, \"tr_fathername\"))\n",
    "            )\n",
    "            \n",
    "            # Add a small delay to ensure everything loads\n",
    "            time.sleep(2)\n",
    "            \n",
    "            data = {'mla_id': mla_id}\n",
    "            \n",
    "            # Map of Hindi labels to English column names\n",
    "            label_map = {\n",
    "                'पिता का नाम': 'father_name',\n",
    "                'जन्‍म तिथि': 'dob',\n",
    "                'जन्‍म स्थान': 'birth_place',\n",
    "                'धर्म': 'religion',\n",
    "                'जाति': 'caste_category',\n",
    "                'शिक्षा': 'qualification',\n",
    "                'विवाह तिथि': 'marriage_date',\n",
    "                'पत्‍नी का नाम': 'spouse_name',\n",
    "                'सन्तान': 'children',\n",
    "                'व्‍यवसाय': 'occupation',\n",
    "                'मुख्यावास': 'permanent_address',\n",
    "                'अस्थाई पता': 'temporary_address',\n",
    "                'मोबाइल नं0': 'mobile_number',\n",
    "                'ई-मेल': 'email'\n",
    "            }\n",
    "            \n",
    "            # Extract basic information\n",
    "            for label, field in label_map.items():\n",
    "                try:\n",
    "                    # Find the row containing the label\n",
    "                    rows = self.driver.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    for row in rows:\n",
    "                        if label in row.text:\n",
    "                            # Find the span in this row\n",
    "                            span = row.find_element(By.TAG_NAME, \"span\")\n",
    "                            data[field] = span.text.strip()\n",
    "                            break\n",
    "                except NoSuchElementException:\n",
    "                    continue\n",
    "            \n",
    "            # Extract political career\n",
    "            try:\n",
    "                career_table = self.driver.find_element(By.ID, \"lbl_political_career\")\n",
    "                career_info = []\n",
    "                rows = career_table.find_elements(By.TAG_NAME, \"tr\")\n",
    "                for row in rows:\n",
    "                    cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                    if len(cells) == 2:\n",
    "                        career_info.append({\n",
    "                            'period': cells[0].text.strip(),\n",
    "                            'description': cells[1].text.strip()\n",
    "                        })\n",
    "                data['political_career'] = json.dumps(career_info, ensure_ascii=False)\n",
    "            except NoSuchElementException:\n",
    "                data['political_career'] = None\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except TimeoutException:\n",
    "            logging.error(f\"Timeout while scraping MLA ID {mla_id}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error scraping MLA ID {mla_id}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def write_to_csv(self, data, first_write=False):\n",
    "        \"\"\"Write data to CSV file.\"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame([data])\n",
    "            if first_write:\n",
    "                df.to_csv(self.output_file, index=False, encoding='utf-8-sig', mode='w')\n",
    "            else:\n",
    "                df.to_csv(self.output_file, index=False, encoding='utf-8-sig', \n",
    "                         mode='a', header=False)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error writing to CSV: {e}\")\n",
    "            return False\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main scraping process.\"\"\"\n",
    "        state = self.load_state()\n",
    "        last_processed_id = state.get('last_processed_id', self.start_id - 1)\n",
    "        processed_ids = set(state.get('processed_ids', []))\n",
    "        first_write = not os.path.exists(self.output_file)\n",
    "\n",
    "        try:\n",
    "            start_from = max(last_processed_id + 1, self.start_id)\n",
    "            print(f\"Starting from MLA ID: {start_from}\")\n",
    "            \n",
    "            for mla_id in tqdm(range(start_from, self.end_id + 1)):\n",
    "                if mla_id in processed_ids:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    data = self.scrape_mla_profile(mla_id)\n",
    "                    if data:\n",
    "                        success = self.write_to_csv(data, first_write)\n",
    "                        if success:\n",
    "                            if first_write:\n",
    "                                first_write = False\n",
    "                            processed_ids.add(mla_id)\n",
    "                            self.save_state(mla_id, processed_ids)\n",
    "                            print(f\"Successfully scraped MLA ID: {mla_id}\")\n",
    "                    else:\n",
    "                        print(f\"Failed to scrape MLA ID: {mla_id}\")\n",
    "\n",
    "                    # Random delay between requests (3-5 seconds)\n",
    "                    time.sleep(3 + random.random() * 2)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing MLA ID {mla_id}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nScraping interrupted!\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {str(e)}\")\n",
    "        finally:\n",
    "            try:\n",
    "                self.driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        print(\"\\nScraping completed!\")\n",
    "        print(f\"Results saved to: {self.output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "711cf084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from MLA ID: 18399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped MLA ID: 18399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|██▉                                         | 1/15 [00:09<02:13,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped MLA ID: 18400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█████▊                                      | 2/15 [00:18<01:57,  9.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped MLA ID: 18401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▊                                   | 3/15 [00:26<01:43,  8.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped MLA ID: 18402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|███████████▋                                | 4/15 [00:35<01:38,  8.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped MLA ID: 18403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████▋                             | 5/15 [00:43<01:24,  8.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped MLA ID: 18404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████▌                          | 6/15 [00:52<01:16,  8.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped MLA ID: 18405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████████████████████▌                       | 7/15 [01:01<01:10,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped MLA ID: 18406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|███████████████████████▍                    | 8/15 [01:09<01:00,  8.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped MLA ID: 18407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████████████████████████▍                 | 9/15 [01:19<00:53,  8.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped MLA ID: 18408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|████████████████████████████▋              | 10/15 [01:28<00:45,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped MLA ID: 18409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████████████████████████████▌           | 11/15 [01:36<00:34,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped MLA ID: 18410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████▍        | 12/15 [01:45<00:26,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped MLA ID: 18411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|█████████████████████████████████████▎     | 13/15 [01:54<00:17,  8.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped MLA ID: 18412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|████████████████████████████████████████▏  | 14/15 [02:03<00:08,  8.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped MLA ID: 18413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 15/15 [02:12<00:00,  8.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping completed!\n",
      "Results saved to: mla_profiles.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        filename='mla_scraping.log'\n",
    "    )\n",
    "    \n",
    "    scraper = MLAFirefoxScraper()\n",
    "    scraper.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d445657e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
